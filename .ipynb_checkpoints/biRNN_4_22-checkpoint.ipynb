{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression: 1 lable range 0-1\n",
    "train_data = pickle.load( open( \"../train_data.p\", \"rb\" ) )\n",
    "train_labels = pickle.load( open( \"../train_labels.p\", \"rb\" ) )\n",
    "test_data = pickle.load( open( \"../test_data.p\", \"rb\" ) )\n",
    "test_labels = pickle.load( open( \"../test_labels.p\", \"rb\" ) )\n",
    "val_data = pickle.load( open( \"../val_data.p\", \"rb\" ) )\n",
    "val_labels = pickle.load( open( \"../val_labels.p\", \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification:  21 labels one hot, 4 features\n",
    "train_data = pickle.load( open( \"../train_data2.p\", \"rb\" ) )\n",
    "train_labels = pickle.load( open( \"../train_labels2.p\", \"rb\" ) )\n",
    "test_data = pickle.load( open( \"../test_data2.p\", \"rb\" ) )\n",
    "test_labels = pickle.load( open( \"../test_labels2.p\", \"rb\" ) )\n",
    "val_data = pickle.load( open( \"../val_data2.p\", \"rb\" ) )\n",
    "val_labels = pickle.load( open( \"../val_labels2.p\", \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subset of 10 samples\n",
    "subset_train_data = train_data[0:100]\n",
    "subset_train_labels = train_labels[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "# learning_rate = 0.001 # optimize this\n",
    "learning_rate = 0.01\n",
    "training_steps = 10000\n",
    "batch_size = 1 # 128\n",
    "display_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "num_input = 4\n",
    "    # 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 23\n",
    "    # 28 # timesteps\n",
    "num_hidden = 8 # ?\n",
    "    # 10 # hidden layer num of features\n",
    "num_classes = 21 # figure out how to do regression\n",
    "    # 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-22f61dfab070>:63: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this resets the graph - re-run tensor-flow specific things after it\n",
    "tf.reset_default_graph() \n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of forward + backward cells\n",
    "    'out': tf.get_variable(\"my_int_variable\", [2*num_hidden, num_classes],\n",
    "  initializer=tf.glorot_uniform_initializer(seed = 23))\n",
    "#     tf.Variable(tf.random_normal([2*num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def BiRNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, num_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, num_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=0.20)\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=0.20)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    try:\n",
    "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                              dtype=tf.float32)\n",
    "    except Exception: # Old TensorFlow version only returns outputs not states\n",
    "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                        dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output then sigmoid result to get output in range [0,1]\n",
    "\n",
    "    # no sigmoid\n",
    "#     print(len(outputs))\n",
    "#     print(outputs[-1])\n",
    "#     1/0\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "    \n",
    "    # sigmoid\n",
    "#     return tf.nn.sigmoid(tf.matmul(outputs[-1], weights['out']) + biases['out'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prediction = BiRNN(X, weights, biases)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=Y))\n",
    "# loss_op = tf.reduce_mean(tf.losses.mean_squared_error(labels=Y, predictions=prediction))\n",
    "\n",
    "# try reduce mean sq without using built in mse fn\n",
    "# loss_op = tf.reduce_mean(tf.square(Y - prediction))\n",
    "\n",
    "\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)  # switch to adam optimizer\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "prediction = tf.nn.softmax(prediction)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "# correct_pred = tf.equal(prediction, Y)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# accuracy = tf.reduce_mean(tf.losses.mean_squared_error(Y, prediction))\n",
    "# accuracy = tf.reduce_mean(abs(Y-prediction))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, x, y):\n",
    "    i = np.random.randint(0,x.shape[0], size=(batch_size))\n",
    "    return np.array(x[i]), np.array(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 4.0089, Validation Accuracy= 1.000\n",
      "Step 100, Minibatch Loss= 3.3258, Validation Accuracy= 1.000\n",
      "Step 200, Minibatch Loss= 3.1618, Validation Accuracy= 1.000\n",
      "Step 300, Minibatch Loss= 3.2193, Validation Accuracy= 1.000\n",
      "Step 400, Minibatch Loss= 2.1683, Validation Accuracy= 1.000\n",
      "Step 500, Minibatch Loss= 2.9130, Validation Accuracy= 1.000\n",
      "Step 600, Minibatch Loss= 3.2285, Validation Accuracy= 1.000\n",
      "Step 700, Minibatch Loss= 1.7716, Validation Accuracy= 1.000\n",
      "Step 800, Minibatch Loss= 2.5496, Validation Accuracy= 1.000\n",
      "Step 900, Minibatch Loss= 1.8190, Validation Accuracy= 1.000\n",
      "Step 1000, Minibatch Loss= 2.6758, Validation Accuracy= 1.000\n",
      "Step 1100, Minibatch Loss= 3.5094, Validation Accuracy= 1.000\n",
      "Step 1200, Minibatch Loss= 2.7748, Validation Accuracy= 1.000\n",
      "Step 1300, Minibatch Loss= 2.6002, Validation Accuracy= 1.000\n",
      "Step 1400, Minibatch Loss= 2.1792, Validation Accuracy= 1.000\n",
      "Step 1500, Minibatch Loss= 3.8445, Validation Accuracy= 1.000\n",
      "Step 1600, Minibatch Loss= 3.1608, Validation Accuracy= 1.000\n",
      "Step 1700, Minibatch Loss= 2.4058, Validation Accuracy= 1.000\n",
      "Step 1800, Minibatch Loss= 3.6797, Validation Accuracy= 1.000\n",
      "Step 1900, Minibatch Loss= 2.8047, Validation Accuracy= 1.000\n",
      "Step 2000, Minibatch Loss= 2.9756, Validation Accuracy= 1.000\n",
      "Step 2100, Minibatch Loss= 2.3394, Validation Accuracy= 1.000\n",
      "Step 2200, Minibatch Loss= 2.9877, Validation Accuracy= 1.000\n",
      "Step 2300, Minibatch Loss= 2.9766, Validation Accuracy= 1.000\n",
      "Step 2400, Minibatch Loss= 2.3823, Validation Accuracy= 1.000\n",
      "Step 2500, Minibatch Loss= 2.8057, Validation Accuracy= 1.000\n",
      "Step 2600, Minibatch Loss= 2.5439, Validation Accuracy= 1.000\n",
      "Step 2700, Minibatch Loss= 2.4761, Validation Accuracy= 1.000\n",
      "Step 2800, Minibatch Loss= 3.7692, Validation Accuracy= 1.000\n",
      "Step 2900, Minibatch Loss= 3.3430, Validation Accuracy= 1.000\n",
      "Step 3000, Minibatch Loss= 3.7671, Validation Accuracy= 1.000\n",
      "Step 3100, Minibatch Loss= 2.4293, Validation Accuracy= 1.000\n",
      "Step 3200, Minibatch Loss= 2.2203, Validation Accuracy= 1.000\n",
      "Step 3300, Minibatch Loss= 3.0577, Validation Accuracy= 1.000\n",
      "Step 3400, Minibatch Loss= 3.4904, Validation Accuracy= 1.000\n",
      "Step 3500, Minibatch Loss= 4.2679, Validation Accuracy= 1.000\n",
      "Step 3600, Minibatch Loss= 2.1174, Validation Accuracy= 1.000\n",
      "Step 3700, Minibatch Loss= 2.7777, Validation Accuracy= 1.000\n",
      "Step 3800, Minibatch Loss= 2.5823, Validation Accuracy= 1.000\n",
      "Step 3900, Minibatch Loss= 3.3070, Validation Accuracy= 1.000\n",
      "Step 4000, Minibatch Loss= 3.0603, Validation Accuracy= 1.000\n",
      "Step 4100, Minibatch Loss= 1.8693, Validation Accuracy= 1.000\n",
      "Step 4200, Minibatch Loss= 2.7763, Validation Accuracy= 1.000\n",
      "Step 4300, Minibatch Loss= 2.1219, Validation Accuracy= 1.000\n",
      "Step 4400, Minibatch Loss= 2.4749, Validation Accuracy= 1.000\n"
     ]
    }
   ],
   "source": [
    "# train_data = subset_train_data\n",
    "# train_labels = subset_train_labels\n",
    "# val_data = subset_train_data\n",
    "# val_labels = subset_train_labels\n",
    "\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = get_batch(batch_size, train_data, train_labels)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "#         batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "#         batch_y = batch_y.reshape((batch_size, num_classes))\n",
    "        \n",
    "        # Run optimization op (backprop)\n",
    "        sess.run([train_op, loss_op, prediction], feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "#             loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})            \n",
    "\n",
    "            loss = sess.run(loss_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "            \n",
    "            #val accuracy\n",
    "            acc = 1\n",
    "\n",
    "\n",
    "\n",
    "#             val_pr\n",
    "#             for i in range(val_labels.shape[0]):\n",
    "#                 val_labels = val_labels.reshape((val_labels.shape[0], num_classes))\n",
    "#                 x = val_data[i].reshape((batch_size,timesteps, num_input))\n",
    "#                 y = val_labels[i].reshape((batch_size,num_classes))\n",
    "#                 val_preds.append(sess.run(accuracy, feed_dict={X: x}))      \n",
    "\n",
    "#             acc = np.mean(val_preds)\n",
    "\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Validation Accuracy= \" + \\\n",
    "                     \"{:.3f}\".format(acc))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#             preds = sess.run(accuracy, feed_dict={X: batch_x})            \n",
    "#             print preds\n",
    "            \n",
    "#             print batch_y\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist val images\n",
    "    accs = []\n",
    "    val_preds = []\n",
    "    for i in range(val_labels.shape[0]/batch_size):\n",
    "        val_labels = val_labels.reshape((val_labels.shape[0], num_classes))\n",
    "        x = val_data[i*batch_size: i*batch_size + batch_size].reshape((batch_size,timesteps, num_input))\n",
    "        y = val_labels[i*batch_size:i*batch_size + batch_size].reshape((batch_size,num_classes))\n",
    "        val_preds.append(sess.run(prediction, feed_dict={X: x}))      \n",
    "        accs.append(sess.run(accuracy, feed_dict={X: x, Y:y}))\n",
    "    \n",
    "    l = np.mean(accs)\n",
    "    print(\"Loss = \" + \"{:.4f}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.argmax(val_preds, axis=2), np.argmax(val_labels, axis=1), alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
