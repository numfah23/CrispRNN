{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression: 1 lable range 0-1\n",
    "train_data = pickle.load( open( \"../train_data_NR.p\", \"rb\" ) )\n",
    "train_labels = pickle.load( open( \"../train_labels_NR.p\", \"rb\" ) )\n",
    "test_data = pickle.load( open( \"../test_data_NR.p\", \"rb\" ) )\n",
    "test_labels = pickle.load( open( \"../test_labels_NR.p\", \"rb\" ) )\n",
    "val_data = pickle.load( open( \"../val_data_NR.p\", \"rb\" ) )\n",
    "val_labels = pickle.load( open( \"../val_labels_NR.p\", \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get subset of 1000 samples\n",
    "subset_train_data = train_data[0:100]\n",
    "subset_train_labels = train_labels[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification:  21 labels one hot, 4 features\n",
    "train_data = pickle.load( open( \"../train_data2.p\", \"rb\" ) )\n",
    "train_labels = pickle.load( open( \"../train_labels2.p\", \"rb\" ) )\n",
    "test_data = pickle.load( open( \"../test_data2.p\", \"rb\" ) )\n",
    "test_labels = pickle.load( open( \"../test_labels2.p\", \"rb\" ) )\n",
    "val_data = pickle.load( open( \"../val_data2.p\", \"rb\" ) )\n",
    "val_labels = pickle.load( open( \"../val_labels2.p\", \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle data\n",
    "d_data = pickle.load(open(\"../d_data.p\", \"rb\"))\n",
    "d_labels = pickle.load(open(\"../d_labels_r.p\", \"rb\"))\n",
    "\n",
    "sub_d_data = d_data[:7000]\n",
    "sub_d_labels = d_labels[:7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = \"NG\" # or \"NG\"\n",
    "forget_b = 1\n",
    "val_step = 10000000\n",
    "# val_step = 100\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.0001\n",
    "# training_steps = 1000000 #10000000 #1000000\n",
    "training_steps = 10000000\n",
    "# training_steps = 100\n",
    "batch_size = 1 # 128\n",
    "display_step = 100000\n",
    "# display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 4    # 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 23   # 28 # timesteps\n",
    "num_hidden = 64  # 10 # hidden layer num of features\n",
    "num_classes = 1  # 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "subset=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subset of 10 samples\n",
    "if(subset>0) :\n",
    "    subset_train_data = train_data[0:subset]\n",
    "    subset_train_labels = train_labels[0:subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this resets the graph - re-run tensor-flow specific things after it\n",
    "tf.reset_default_graph() \n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of forward + backward cells\n",
    "    'out': tf.get_variable(\"my_int_variable\", [2*num_hidden, num_classes], \n",
    "  initializer=tf.glorot_uniform_initializer(seed = 23))\n",
    "#     tf.Variable(tf.random_normal([2*num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "def BiRNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, num_input)\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, num_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define lstm cells with tensorflow\n",
    "#     lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=forget_b)    # Forward direction cell\n",
    "#     lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=forget_b)    # Backward direction cell\n",
    "\n",
    "        ################ try relu activation instead\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=forget_b, activation=tf.nn.relu)\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=forget_b, activation=tf.nn.relu)\n",
    "\n",
    "    try:                            # Get lstm cell output\n",
    "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                              dtype=tf.float32)\n",
    "    except Exception:               # Old TensorFlow version only returns outputs not states\n",
    "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                        dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output then sigmoid result to get output in range [0,1]\n",
    "\n",
    "    # no sigmoid\n",
    "    #return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "    \n",
    "    # sigmoid\n",
    "    return tf.nn.sigmoid(tf.matmul(outputs[-1], weights['out']) + biases['out'])\n",
    "\n",
    "prediction = BiRNN(X, weights, biases)\n",
    "\n",
    "# Define loss and optimizer for classification:\n",
    "#loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=Y))\n",
    "# for regression:\n",
    "loss_op = tf.reduce_mean(tf.losses.mean_squared_error(labels=Y, predictions=prediction))\n",
    "\n",
    "# loss_op = tf.reduce_mean(tf.square(Y - prediction))            # try reduce mean sq without using built in mse fn\n",
    "\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)  # switch to adam optimizer\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "# prediction = tf.nn.softmax(prediction)\n",
    "# prediction = tf.argmax(tf.nn.softmax(prediction), 1)\n",
    "\n",
    "# this is for classification\n",
    "# correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "# correct_pred = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "#for regression\n",
    "accuracy = tf.reduce_mean(tf.losses.mean_squared_error(Y, prediction))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, x, y):\n",
    "    i = np.random.randint(0,x.shape[0], size=(batch_size))\n",
    "    return np.array(x[i]), np.array(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 0.0833, Validation Loss= 0.06163\n"
     ]
    }
   ],
   "source": [
    "# train and val on same data\n",
    "# train_data = subset_train_data\n",
    "# train_labels = subset_train_labels\n",
    "# val_data = subset_train_data\n",
    "# val_labels = subset_train_labels\n",
    "\n",
    "with tf.Session() as sess:               # Start training\n",
    "    sess.run(init)                       # Run the initializer\n",
    "    \n",
    "    # things to save during display step + val step\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = get_batch(batch_size, train_data, train_labels)\n",
    "        \n",
    "        # Reshape data to get 28 seq of 28 elements        \n",
    "        # comment out with 3 or 21 labels + no-gene.... need for regression\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        batch_y = batch_y.reshape((batch_size, num_classes))\n",
    "        \n",
    "        # Run optimization op (backprop) in every training step\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        \n",
    "        # in display step, clac train_losses\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            t_loss = sess.run(loss_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "            train_losses.append(t_loss)\n",
    "        \n",
    "        # in val step, calc val_losses\n",
    "        if step % val_step == 0 or step == 1:\n",
    "            vl_temp = []\n",
    "            for i in range(val_labels.shape[0]/batch_size):\n",
    "#               val_labels = val_labels.reshape((val_labels.shape[0], num_classes)) # < don't need this??\n",
    "                x = val_data[i*batch_size: i*batch_size + batch_size].reshape((batch_size,timesteps, num_input))\n",
    "                y = val_labels[i*batch_size:i*batch_size + batch_size].reshape((batch_size,num_classes))\n",
    "                vl_temp.append(sess.run(loss_op, feed_dict={X: x, Y:y}))\n",
    "            v_loss = np.mean(vl_temp)\n",
    "            val_losses.append(v_loss)\n",
    "\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(t_loss) + \", Validation Loss= \" + \"{:.5f}\".format(v_loss))\n",
    "        \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    \n",
    "    \n",
    "#     # Calculate accuracy and predictions for validation set\n",
    "#     val_preds = []\n",
    "#     val_accs_final = []\n",
    "#     val_losses_final = []\n",
    "\n",
    "#     for i in range(val_labels.shape[0]/batch_size):\n",
    "# #       val_labels = val_labels.reshape((val_labels.shape[0], num_classes)) # < don't need this??\n",
    "#         x = val_data[i*batch_size: i*batch_size + batch_size].reshape((batch_size,timesteps, num_input))\n",
    "#         y = val_labels[i*batch_size:i*batch_size + batch_size].reshape((batch_size,num_classes))\n",
    "#         val_preds.append(sess.run(prediction, feed_dict={X: x}))      \n",
    "#         val_accs_final.append(sess.run(accuracy, feed_dict={X: x, Y:y}))\n",
    "#         val_losses_final.append(sess.run(loss_op, feed_dict={X: x, Y:y}))\n",
    "    \n",
    "#     val_acc_f = np.mean(val_accs_final)\n",
    "#     val_loss_f = np.mean(val_losses_final)\n",
    "    \n",
    "#     print(\"Final Validation Accuracy = \" + \"{:.4f}\".format(val_acc_f))\n",
    "#     print(\"Final Validation Loss = \" + \"{:.4f}\".format(val_loss_f))\n",
    "    \n",
    "    \n",
    "    #######################################################################################################\n",
    "    # Calculate accuracy and predictions for validation set\n",
    "    val_preds = []\n",
    "    val_accs_final = []\n",
    "    val_losses_final = []\n",
    "\n",
    "    for i in range(val_labels.shape[0]/batch_size):\n",
    "#         val_labels = val_labels.reshape((val_labels.shape[0], num_classes)) # < don't need this??\n",
    "        x = val_data[i*batch_size: i*batch_size + batch_size].reshape((batch_size,timesteps, num_input))\n",
    "        y = val_labels[i*batch_size:i*batch_size + batch_size].reshape((batch_size,num_classes))\n",
    "        val_preds.append(sess.run(prediction, feed_dict={X: x}))      \n",
    "        val_accs_final.append(sess.run(accuracy, feed_dict={X: x, Y:y}))\n",
    "        val_losses_final.append(sess.run(loss_op, feed_dict={X: x, Y:y}))\n",
    "    \n",
    "    val_acc_f = np.mean(val_accs_final)\n",
    "    val_loss_f = np.mean(val_losses_final)\n",
    "    \n",
    "    print(\"Final Validtion Accuracy = \" + \"{:.4f}\".format(val_acc_f))\n",
    "    print(\"Final Validation Loss = \" + \"{:.4f}\".format(val_loss_f))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #######################################################################################################\n",
    "    # Calculate accuracy and predictions for testing set\n",
    "    test_preds = []\n",
    "    test_accs_final = []\n",
    "    test_losses_final = []\n",
    "\n",
    "    for i in range(test_labels.shape[0]/batch_size):\n",
    "#         val_labels = val_labels.reshape((val_labels.shape[0], num_classes)) # < don't need this??\n",
    "        x = test_data[i*batch_size: i*batch_size + batch_size].reshape((batch_size,timesteps, num_input))\n",
    "        y = test_labels[i*batch_size:i*batch_size + batch_size].reshape((batch_size,num_classes))\n",
    "        test_preds.append(sess.run(prediction, feed_dict={X: x}))      \n",
    "        test_accs_final.append(sess.run(accuracy, feed_dict={X: x, Y:y}))\n",
    "        test_losses_final.append(sess.run(loss_op, feed_dict={X: x, Y:y}))\n",
    "    \n",
    "    test_acc_f = np.mean(test_accs_final)\n",
    "    test_loss_f = np.mean(test_losses_final)\n",
    "    \n",
    "    print(\"Final Test Accuracy = \" + \"{:.4f}\".format(test_acc_f))\n",
    "    print(\"Final Test Loss = \" + \"{:.4f}\".format(test_loss_f))\n",
    "    \n",
    "    #######################################################################################################\n",
    "    # Calculate accuracy and predictions for doench data\n",
    "    d_preds = []\n",
    "    d_accs_final = []\n",
    "    d_losses_final = []\n",
    "    d_data = sub_d_data\n",
    "    d_labels = sub_d_labels\n",
    "\n",
    "    for i in range(d_labels.shape[0]/batch_size):\n",
    "#         val_labels = val_labels.reshape((val_labels.shape[0], num_classes)) # < don't need this??\n",
    "        x = d_data[i*batch_size: i*batch_size + batch_size].reshape((batch_size,timesteps, num_input))\n",
    "        y = d_labels[i*batch_size:i*batch_size + batch_size].reshape((batch_size,num_classes))\n",
    "        d_preds.append(sess.run(prediction, feed_dict={X: x}))      \n",
    "        d_accs_final.append(sess.run(accuracy, feed_dict={X: x, Y:y}))\n",
    "        d_losses_final.append(sess.run(loss_op, feed_dict={X: x, Y:y}))\n",
    "    \n",
    "    d_acc_f = np.mean(d_accs_final)\n",
    "    d_loss_f = np.mean(d_losses_final)\n",
    "    \n",
    "    print(\"Final Doench Accuracy = \" + \"{:.4f}\".format(d_acc_f))\n",
    "    print(\"Final Doench Loss = \" + \"{:.4f}\".format(d_loss_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for val\n",
    "preds = [v[0] for v in val_preds]\n",
    "preds_1d = np.array(preds).reshape(len(preds),)\n",
    "vals_1d = val_labels.reshape(val_labels.shape[0],)\n",
    "\n",
    "plt.scatter(vals_1d, preds_1d, alpha=0.2)\n",
    "\n",
    "z = np.polyfit(vals_1d, preds_1d, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(vals_1d, p(vals_1d), \"r\")\n",
    "\n",
    "plt.title(\"Predictions for validation set: regression, lr = 0.0001, iter = 10000000, relu\")\n",
    "# plt.title(\"Predictions for 100 training set: regression, lr = 0.001, iter = 100000, relu\")\n",
    "\n",
    "\n",
    "plt.ylabel(\"predicted effect\")\n",
    "plt.xlabel(\"actual effect\")\n",
    "plt.show()\n",
    "plt.savefig(\"val_preds_r_relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "st.spearmanr(preds_1d, vals_1d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for test\n",
    "tpreds = [v[0] for v in test_preds]\n",
    "tpreds_1d = np.array(tpreds).reshape(len(tpreds),)\n",
    "tlabs_1d = test_labels.reshape(test_labels.shape[0],)\n",
    "\n",
    "plt.scatter(tlabs_1d, tpreds_1d, alpha=0.2)\n",
    "\n",
    "z = np.polyfit(tlabs_1d, tpreds_1d, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(tlabs_1d, p(tlabs_1d), \"r\")\n",
    "\n",
    "plt.title(\"Predictions for testing set: regression, lr = 0.0001, iter = 10000000, relu\")\n",
    "plt.ylabel(\"predicted effect\")\n",
    "plt.xlabel(\"actual effect\")\n",
    "plt.show()\n",
    "plt.savefig(\"test_preds_r_relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.spearmanr(tpreds_1d, tlabs_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for doench\n",
    "dpreds = [v[0] for v in d_preds]\n",
    "dpreds_1d = np.array(dpreds).reshape(len(dpreds),)\n",
    "dlabs_1d = d_labels.reshape(d_labels.shape[0],)\n",
    "\n",
    "plt.scatter(dlabs_1d, dpreds_1d, alpha=0.2)\n",
    "\n",
    "z = np.polyfit(dlabs_1d, dpreds_1d, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(dlabs_1d, p(dlabs_1d), \"r\")\n",
    "\n",
    "plt.title(\"Predictions for doench data: regression, lr = 0.0001, iter = 10000000, relu\")\n",
    "plt.ylabel(\"predicted effect\")\n",
    "plt.xlabel(\"actual effect\")\n",
    "plt.show()\n",
    "plt.savefig(\"doench_preds_r_relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.spearmanr(dpreds_1d, dlabs_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training loss over time\n",
    "plt.scatter(np.arange(0, training_steps+display_step, display_step), train_losses)\n",
    "plt.xlabel(\"training iteration\")\n",
    "plt.ylabel(\"training loss\")\n",
    "plt.title(\"MSE loss for training data: regression, lr = 0.0001, iter = 10000000, relu\")\n",
    "plt.savefig(\"train_loss_r_relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### saving results summary for each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. overall_results\n",
    "# columns = gene(G/NG) , num_classes(1,3,21), num_hidden(8,32,128), lr(0.1,0.01,0.001,0.0001), \n",
    "    # forget(0,0.5,1), weight(0,1,2), bias(0.01,0.001), train_loss, val_loss, train_acc, val_acc\n",
    "\n",
    "output_file = open(\"../r_results_relu_1e7.csv\", \"a\")\n",
    "\n",
    "if(subset>0) :\n",
    "#output_lst = [G, num_classes, num_hidden, learning_rate, forget_b, weight_mean, bias_mean, train_losses[-1], val_losses[-1], val_acc]\n",
    "    output_lst = [G, num_classes, num_hidden, learning_rate, forget_b, train_losses[-1], val_loss_f, val_acc_f, \"subset\", subset]\n",
    "else :\n",
    "    output_lst = [G, num_classes, num_hidden, learning_rate, forget_b, train_losses[-1], val_loss_f, val_acc_f]\n",
    "    \n",
    "#output_lst = [G, num_classes, num_hidden, learning_rate, forget_b, weight_mean, bias_mean, train_losses[-1], val_loss_f, val_acc_f]\n",
    "output_file.writelines(\",\".join([str(i) for i in output_lst])+\"\\n\")\n",
    "output_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if(subset>0) :\n",
    "#     fname = [\"subset\", subset, G, num_classes, num_hidden, learning_rate, forget_b]\n",
    "# else :\n",
    "#     fname = [G, num_classes, num_hidden, learning_rate, forget_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a. prediction and true labels for val\n",
    "# (preds_1d, vals_1d) instead of preds, vl\n",
    "# (dpreds_1d, dlabs_1d), (tpreds_1d, tlabs_1d)\n",
    "\n",
    "pv_fname = \"../rfinal_val_preds_relu1e7\" + \"_\".join([str(i) for i in output_lst]) + \".csv\"\n",
    "pred_vlabels = np.vstack((np.array(preds_1d), vals_1d)).T\n",
    "np.savetxt(pv_fname, pred_vlabels, delimiter=\",\")\n",
    "\n",
    "# 2b. prediction and true labels for test\n",
    "pt_fname = \"../rfinal_test_preds_relu1e7\" + \"_\".join([str(i) for i in output_lst]) + \".csv\"\n",
    "pred_tlabels = np.vstack((np.array(tpreds_1d), tlabs_1d)).T\n",
    "np.savetxt(pt_fname, pred_tlabels, delimiter=\",\")\n",
    "\n",
    "# 2c. prediction and true labels for doench\n",
    "pd_fname = \"../rfinal_d_preds_relu1e7\" + \"_\".join([str(i) for i in output_lst]) + \".csv\"\n",
    "pred_dlabels = np.vstack((np.array(dpreds_1d), dlabs_1d)).T\n",
    "np.savetxt(pd_fname, pred_dlabels, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. training loss over time\n",
    "tl_fname = \"../rfinal_train_loss_relu1e7\" + \"_\".join([str(i) for i in output_lst]) + \".csv\"\n",
    "train_loss_time = np.vstack((np.arange(0, training_steps+display_step, display_step), train_losses)).T\n",
    "np.savetxt(tl_fname, train_loss_time, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. validation loss over time\n",
    "vl_fname = \"../rfinal_val_loss_relu1e7\" + \"_\".join([str(i) for i in output_lst]) + \".csv\"\n",
    "val_loss_time = np.vstack((np.arange(0, training_steps+val_step, val_step), val_losses)).T\n",
    "np.savetxt(vl_fname, val_loss_time, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
